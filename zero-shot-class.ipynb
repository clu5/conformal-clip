{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e0c95a7-783a-4bee-83bd-7d3ccb1dc11d",
   "metadata": {},
   "source": [
    "# CLIP with conformal prediction\n",
    "\n",
    "Comparing prediciton set size across different datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "920f7d48-46be-4568-b372-5da6ba8b09db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt; plt.style.use('bmh')\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "assert torch.cuda.is_available()\n",
    "import open_clip\n",
    "from tqdm import tqdm\n",
    "import wilds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb051a6a-678d-4cd2-9454-6d7d0b946068",
   "metadata": {},
   "source": [
    "## datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbb9a74e-6041-4965-8ea0-5b606cb7e832",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('../data')\n",
    "cifar_train = datasets.CIFAR100(root=data_dir, train=True)\n",
    "cifar_test = datasets.CIFAR100(root=data_dir, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db967beb-5290-4ab8-bb31-67643f4deab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_classes = tuple(cifar_test.classes)\n",
    "cifar_class_map = dict(map(reversed, cifar_test.class_to_idx.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76d6a422-e41d-4251-950a-ee89b31cc7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DS(Dataset):\n",
    "    def __init__(self, images, labels, transforms=None, target_transforms=None):\n",
    "        assert len(images) == len(labels)\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transforms = transforms\n",
    "        self.target_transforms = target_transforms\n",
    "        \n",
    "    def __getitem__(self, k):\n",
    "        x = self.images[k]\n",
    "        y = self.labels[k]\n",
    "        if self.transforms is not None:\n",
    "            x = self.transforms(x)\n",
    "        if self.target_transforms is not None:\n",
    "            y = self.target_transforms(y)\n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7794067-13ae-4e1b-9185-18c0e2d12aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_test_dataset = DS(\n",
    "    cifar_test.data, cifar_test.targets,\n",
    "    transforms=transforms.Compose([\n",
    "        # transforms.ToTensor(),\n",
    "        # transforms.Normalize((0.5071, 0.4865, 0.4409), (0.2673, 0.2564, 0.2762)),\n",
    "        transforms.ToPILImage(),\n",
    "    ]), \n",
    ")\n",
    "\n",
    "loader_params = dict(batch_size=16, shuffle=False, pin_memory=True, num_workers=8)\n",
    "cifar_test_loader = DataLoader(cifar_test_dataset, **loader_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "194c2c79-b9f1-48ca-b0e5-689df7785c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `wild_dataset.get_subset` not found.\n"
     ]
    }
   ],
   "source": [
    "wild_dataset.get_subset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "549f6111-6521-4d39-96c2-c7608fec3dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "wild_dataset = wilds.get_dataset(dataset='iwildcam', root_dir=data_dir/'wilds')\n",
    "wild_test_id = wild_dataset.get_subset('id_test')\n",
    "wild_test_od = wild_dataset.get_subset('test', frac=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73459975-ce42-4f78-9c71-a86c3c42e6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "wild_df = pd.read_csv(wild_dataset._data_dir / 'categories.csv')\n",
    "wild_class_map = dict(wild_df[['y', 'name']].values)\n",
    "wild_classes = tuple(wild_class_map.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b86bbbdf-da46-4a7a-80e2-f06f29330f93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8558, 8154)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wild_test_od), len(wild_test_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f272e69-4ec3-4fd2-afee-d3e71d55b072",
   "metadata": {},
   "source": [
    "## CLIP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d19667d-b06f-49e1-aa13-e0756f917e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = open_clip.get_tokenizer('ViT-B-32-quickgelu')\n",
    "model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32-quickgelu', pretrained='laion400m_e32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e9e39a3-d5e5-4006-be2e-d38621b975c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [14:52<00:00, 11.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR100 accuracy: 68.6%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cifar_true_class = []\n",
    "cifar_pred_class = []\n",
    "cifar_pred_scores = []\n",
    "\n",
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "    prompt = tokenizer(['This is an image of a ' + c for c in cifar_classes])\n",
    "    text_features = model.encode_text(prompt)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    for i in tqdm(range(len(cifar_test_dataset))):\n",
    "        x, y = cifar_test_dataset[i]\n",
    "        image_features = model.encode_image(preprocess(x).unsqueeze(0))\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        scores = (image_features @ text_features.T).softmax(dim=-1)\n",
    "        \n",
    "        cifar_true_class.append(y)\n",
    "        cifar_pred_class.append(scores.argmax().item())\n",
    "        cifar_pred_scores.append(scores.detach().cpu().numpy().squeeze())\n",
    "        \n",
    "cifar_true_class = np.asarray(cifar_true_class)\n",
    "cifar_pred_class = np.asarray(cifar_pred_class)\n",
    "cifar_pred_scores = np.asarray(cifar_pred_scores)\n",
    "\n",
    "cifar_acc = (cifar_true_class == cifar_pred_class).sum() / cifar_true_class.shape[0]\n",
    "print(f'CIFAR100 accuracy: {cifar_acc:.1%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b654081-241d-4f8d-b6f0-743a1de837ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████████████████████████████████████████████████████████▉                                                                                    | 3292/8154 [06:51<26:33,  3.05it/s]"
     ]
    }
   ],
   "source": [
    "wild_id_true_class = []\n",
    "wild_id_pred_class = []\n",
    "wild_id_pred_scores = []\n",
    "\n",
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "    prompt = tokenizer(['This is an image of a ' + c for c in wild_classes])\n",
    "    text_features = model.encode_text(prompt)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    for i in tqdm(range(len(wild_test_id))):\n",
    "        x, y, _ = wild_test_id[i]\n",
    "        image_features = model.encode_image(preprocess(x).unsqueeze(0))\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        scores = (image_features @ text_features.T).softmax(dim=-1)\n",
    "        \n",
    "        wild_id_true_class.append(y)\n",
    "        wild_id_pred_class.append(scores.argmax().item())\n",
    "        wild_id_pred_scores.append(scores.detach().cpu().numpy().squeeze())\n",
    "        \n",
    "wild_id_true_class = np.asarray(wild_id_true_class)\n",
    "wild_id_pred_class = np.asarray(wild_id_pred_class)\n",
    "wild_id_pred_scores = np.asarray(wild_id_pred_scores)\n",
    "\n",
    "wild_od_true_class = []\n",
    "wild_od_pred_class = []\n",
    "wild_od_pred_scores = []\n",
    "\n",
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "    prompt = tokenizer(['This is an image of a ' + c for c in wild_classes])\n",
    "    text_features = model.encode_text(prompt)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    for i in tqdm(range(len(wild_test_id))):\n",
    "        x, y, _ = wild_test_id[i]\n",
    "        image_features = model.encode_image(preprocess(x).unsqueeze(0))\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        scores = (image_features @ text_features.T).softmax(dim=-1)\n",
    "        \n",
    "        wild_od_true_class.append(y)\n",
    "        wild_od_pred_class.append(scores.argmax().item())\n",
    "        wild_od_pred_scores.append(scores.detach().cpu().numpy().squeeze())\n",
    "        \n",
    "wild_od_true_class = np.asarray(wild_od_true_class)\n",
    "wild_od_pred_class = np.asarray(wild_od_pred_class)\n",
    "wild_od_pred_scores = np.asarray(wild_od_pred_scores)\n",
    "\n",
    "wild_id_acc = (wild_id_true_class == wild_id_pred_class).sum() / wild_id_true_class.shape[0]\n",
    "print(f'WILD ID accuracy: {wild_id_acc:.1%}')\n",
    "wild_od_acc = (wild_od_true_class == wild_od_pred_class).sum() / wild_od_true_class.shape[0]\n",
    "print(f'WILD OD accuracy: {wild_od_acc:.1%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c3ea22-a965-47fd-a048-1fa00510ca74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74f2a3f-01b2-467c-ade6-79a53ace0107",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0085273e-9632-4137-bbb7-19978237668d",
   "metadata": {},
   "source": [
    "## Conformal prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403550b5-9c59-412c-8813-191084b33f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quantile(scores, targets, alpha=0.1):\n",
    "    n = torch.tensor(targets.size(0))\n",
    "    score_dist = torch.take_along_dim(1 - scores, targets.unsqueeze(1), 1).flatten()\n",
    "    qhat = torch.quantile(score_dist, torch.ceil((n + 1) * (1 - alpha)) / n, interpolation=\"higher\")\n",
    "    return qhat\n",
    "\n",
    "\n",
    "def make_prediction_sets(scores, qhat, allow_empty_sets=False):\n",
    "    n = scores.size(0)\n",
    "    elements_mask = scores >= (1 - qhat)\n",
    "    if not allow_empty_sets:\n",
    "        elements_mask[torch.arange(n), scores.argmax(1)] = True\n",
    "    return elements_mask\n",
    "\n",
    "def get_coverage(psets, targets, precision=None):\n",
    "    psets = psets.clone().detach()\n",
    "    targets = targets.clone().detach()\n",
    "    n = psets.shape[0]\n",
    "    coverage = psets[torch.arange(n), targets].float().mean().item()\n",
    "    if precision is not None:\n",
    "        coverage = round(coverage, precision)\n",
    "    return coverage\n",
    "\n",
    "\n",
    "def get_size(psets, precision=1):\n",
    "    psets = psets.clone().detach()\n",
    "    size = psets.sum(1).float().mean().item()\n",
    "    if precision is not None:\n",
    "        size = round(size, precision)\n",
    "    return size\n",
    "\n",
    "\n",
    "def get_coverage_by_class(psets, targets, num_classes):\n",
    "    psets = psets.clone().detach()\n",
    "    targets = targets.clone().detach()\n",
    "    results = {}\n",
    "    for c in range(num_classes):\n",
    "        index = targets == c\n",
    "        psets_c = psets[index]\n",
    "        targets_c = targets[index]\n",
    "        results[c] = get_coverage(psets_c, targets_c)\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_efficiency_by_class(psets, targets, num_classes):\n",
    "    psets = psets.clone().detach()\n",
    "    targets = targets.clone().detach()\n",
    "    sizes = psets.sum(1)\n",
    "    results = {}\n",
    "    for c in range(num_classes):\n",
    "        index = targets == c\n",
    "        psets_c = psets[index]\n",
    "        results[c] = get_size(psets_c)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072bad21-5119-4c6b-a7b5-8b512a47e801",
   "metadata": {},
   "outputs": [],
   "source": [
    "frac = 0.1\n",
    "alpha = 0.1\n",
    "\n",
    "cifar_n = int(round(frac * len(cifar_pred_scores)))\n",
    "cifar_cal_scores = torch.tensor(cifar_pred_scores[:cifar_n])\n",
    "cifar_cal_targets = torch.tensor(cifar_true_class[:cifar_n])\n",
    "cifar_val_scores = torch.tensor(cifar_pred_scores[cifar_n:])\n",
    "cifar_val_targets = torch.tensor(cifar_true_class[cifar_n:])\n",
    "\n",
    "cifar_qhat = get_quantile(cifar_cal_scores, cifar_cal_targets, alpha=alpha)\n",
    "cifar_psets = make_prediction_sets(cifar_val_scores, cifar_qhat)\n",
    "\n",
    "print(f'CIFAR100 coverage: {get_coverage(cifar_psets, cifar_val_targets):.1%}')\n",
    "print(f'CIFAR100 set size: {get_size(cifar_psets):.1f}')\n",
    "\n",
    "wild_id_n = int(round(frac * len(wild_id_pred_scores)))\n",
    "wild_id_cal_scores = torch.tensor(wild_id_pred_scores[:wild_id_n])\n",
    "wild_id_cal_targets = torch.tensor(wild_id_true_class[:wild_id_n])\n",
    "wild_id_val_scores = torch.tensor(wild_id_pred_scores[wild_id_n:])\n",
    "wild_id_val_targets = torch.tensor(wild_id_true_class[wild_id_n:])\n",
    "\n",
    "wild_id_qhat = get_quantile(wild_id_cal_scores, wild_id_cal_targets, alpha=alpha)\n",
    "wild_id_psets = make_prediction_sets(wild_id_val_scores, wild_id_qhat)\n",
    "\n",
    "print(f'WILD ID coverage: {get_coverage(wild_id_psets, wild_id_val_targets):.1%}')\n",
    "print(f'WILD ID set size: {get_size(wild_id_psets):.1f}')\n",
    "\n",
    "wild_od_n = int(round(frac * len(wild_od_pred_scores)))\n",
    "wild_od_cal_scores = torch.tensor(wild_od_pred_scores[:wild_od_n])\n",
    "wild_od_cal_targets = torch.tensor(wild_od_true_class[:wild_od_n])\n",
    "wild_od_val_scores = torch.tensor(wild_od_pred_scores[wild_od_n:])\n",
    "wild_od_val_targets = torch.tensor(wild_od_true_class[wild_od_n:])\n",
    "\n",
    "wild_od_qhat = get_quantile(wild_od_cal_scores, wild_od_cal_targets, alpha=alpha)\n",
    "wild_od_psets = make_prediction_sets(wild_od_val_scores, wild_od_qhat)\n",
    "\n",
    "print(f'WILD OD coverage: {get_coverage(wild_od_psets, wild_od_val_targets):.1%}')\n",
    "print(f'WILD OD set size: {get_size(wild_od_psets):.1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a2e16b-3d3d-45be-a4a1-723ff48d1d80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484ff946-3413-495a-beb6-ff65985186e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing random state for reproducibility\n",
    "np.random.seed(19680801)\n",
    "\n",
    "\n",
    "# fake data\n",
    "fs = 10  # fontsize\n",
    "pos = [1, 2, 4, 5, 7, 8]\n",
    "data = [np.random.normal(0, std, size=100) for std in pos]\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, ncols=5, figsize=(10, 6))\n",
    "\n",
    "axs[0, 0].violinplot(data, pos, points=20, widths=0.3,\n",
    "                     showmeans=True, showextrema=True, showmedians=True)\n",
    "axs[0, 0].set_title('Custom violinplot 1', fontsize=fs)\n",
    "\n",
    "axs[0, 1].violinplot(data, pos, points=40, widths=0.5,\n",
    "                     showmeans=True, showextrema=True, showmedians=True,\n",
    "                     bw_method='silverman')\n",
    "axs[0, 1].set_title('Custom violinplot 2', fontsize=fs)\n",
    "\n",
    "axs[0, 2].violinplot(data, pos, points=60, widths=0.7, showmeans=True,\n",
    "                     showextrema=True, showmedians=True, bw_method=0.5)\n",
    "axs[0, 2].set_title('Custom violinplot 3', fontsize=fs)\n",
    "\n",
    "axs[0, 3].violinplot(data, pos, points=60, widths=0.7, showmeans=True,\n",
    "                     showextrema=True, showmedians=True, bw_method=0.5,\n",
    "                     quantiles=[[0.1], [], [], [0.175, 0.954], [0.75], [0.25]])\n",
    "axs[0, 3].set_title('Custom violinplot 4', fontsize=fs)\n",
    "\n",
    "axs[0, 4].violinplot(data[-1:], pos[-1:], points=60, widths=0.7,\n",
    "                     showmeans=True, showextrema=True, showmedians=True,\n",
    "                     quantiles=[0.05, 0.1, 0.8, 0.9], bw_method=0.5)\n",
    "axs[0, 4].set_title('Custom violinplot 5', fontsize=fs)\n",
    "\n",
    "axs[1, 0].violinplot(data, pos, points=80, vert=False, widths=0.7,\n",
    "                     showmeans=True, showextrema=True, showmedians=True)\n",
    "axs[1, 0].set_title('Custom violinplot 6', fontsize=fs)\n",
    "\n",
    "axs[1, 1].violinplot(data, pos, points=100, vert=False, widths=0.9,\n",
    "                     showmeans=True, showextrema=True, showmedians=True,\n",
    "                     bw_method='silverman')\n",
    "axs[1, 1].set_title('Custom violinplot 7', fontsize=fs)\n",
    "\n",
    "axs[1, 2].violinplot(data, pos, points=200, vert=False, widths=1.1,\n",
    "                     showmeans=True, showextrema=True, showmedians=True,\n",
    "                     bw_method=0.5)\n",
    "axs[1, 2].set_title('Custom violinplot 8', fontsize=fs)\n",
    "\n",
    "axs[1, 3].violinplot(data, pos, points=200, vert=False, widths=1.1,\n",
    "                     showmeans=True, showextrema=True, showmedians=True,\n",
    "                     quantiles=[[0.1], [], [], [0.175, 0.954], [0.75], [0.25]],\n",
    "                     bw_method=0.5)\n",
    "axs[1, 3].set_title('Custom violinplot 9', fontsize=fs)\n",
    "\n",
    "axs[1, 4].violinplot(data[-1:], pos[-1:], points=200, vert=False, widths=1.1,\n",
    "                     showmeans=True, showextrema=True, showmedians=True,\n",
    "                     quantiles=[0.05, 0.1, 0.8, 0.9], bw_method=0.5)\n",
    "axs[1, 4].set_title('Custom violinplot 10', fontsize=fs)\n",
    "\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set_yticklabels([])\n",
    "\n",
    "fig.suptitle(\"Violin Plotting Examples\")\n",
    "fig.subplots_adjust(hspace=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68d122f-a079-41b7-83b4-7ad580450f65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8465a6bb-6888-41c8-af1d-8cb234f3d748",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
